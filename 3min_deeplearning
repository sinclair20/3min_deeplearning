'''CNN'''
'''
CNN 모델은 기본적으로 컨볼루션 계층(합성곱 계층)과 풀링계층 으로 구성됨.
이 계층들을 얼마나 많이 또 어떠한 방식으로 쌓느냐에 따라 성능차이와 풀수있는 문제가 달라짐

2D 컨볼루션의 경우 
2차원의 평면 행렬에서 지정한 영역의 값들을 하나의 값으로 압축하는 것
단, 하나의 값으로 압축할때 컨볼루션 계층은 가중치와 편향을 적용하고 풀링계층은 단순히 값들 중 
하나를 선택해서 가져오는 방식

윈도우: 지정한 크기의 영역
윈도우의 값을 오른쪽 그리고 아래쪽으로 한칸씩 움직이면서 은닉층을 완성. 움직이는 크기 또한 변경가능.
몇칸씩 움직일지 정하는 값을 '스트라이드' 라고 함

이렇게 입력층의 윈도우를 은닉층의 뉴런하나로 압축할때, 컨볼루션 계층에서는 윈도우 크기만큼의 가중치와 
1개의 편향을 적용.
ex) 윈도우 크기가 3x3이면 , 3x3 개의 가중치와 1개의 편향이 필요.

3x3 개의 가중치와 1개의 편향을 커널 또는 필터라고 하며, 이 커널은 해당 은닉층을 만들기 위한 모든 윈도우에
공통으로 적용됨

=> 이것이 바로 CNN의 가장 중요한 특징. ex) 입력층이 28x28 개라고 했을때, 기본 신경망으로 모든 뉴련을 연결한다면
784개의 가중치를 찾아내야 하지만 컨볼루션 계층에서는 3x3개인 9개의 가중치만 찾아내면 됨.
따라서 계산량에 매우적어져 학습이 빠르고 효율적으로 이루어짐
but, 이렇게 하면 복잡한 특징을 가진 이미지들을 분석하기에 부족하므로 보통 커널을 여러개 사용
커널의 크기나 개수 역시 하이퍼파라미터의 하나로써 분석하고자 하는 내용에 따라 커널의 개수를 정하는 일 매우 중요
'''


# 앞에서 사용한 MNIST 데이터를 CNN으로 학습시키는 모델 만들기

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data


mnist = input_data.read_data_sets('./mnist/data/', one_hot=True)
# 앞장에서 만든 모델에서는 입력값을 28x28짜리 차원 하나로 구성했지만 CNN 모델에서는 앞서 설명한 것처럼 2차원 평면으로
# 구성하므로 다음처럼 조금더 직관적인 형태로 구성가능

# X의 첫번째 차원인 None은 입력데이터의 개수 그리고 마지막 차원인 1은 특징의 개수로 MNIST 데이터는 회색조 이미지라 채널에 색상이 한개 뿐이므로 1을 사용.
# 그리고 출력값인 10개의 분류와, 드롭아웃을 위한 keep_prob 플레이스 홀더 정의.
X = tf.placeholder(tf.float32, [None, 28, 28, 1])
Y = tf.placeholder(tf.float32, [None, 10])
keep_prob = tf.placeholder(tf.float32)


# 첫번째 CNN 계층 구성
# 3x3 크기의 커널을 가진 컨볼루션 계층 만듬.
# 커널에 사용할 가중치 변수와 tf.nn.conv2d 함수를 사용하면 간단하게 구성가능
W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))

# 입력층 X와 첫번째 계층의 가중치 W1을 가지고 오른쪽과 아래쪽으로 한칸씩 움직이는 32개의 커널을 가진
# 컨볼루션 계층을 만들겠다는 코드
# padding='SAME'은 커널 슬라이딩 시 이미지의 가장 외곽에서 한칸 밖으로 움직이는 옵션
# 이로인해 이미지의 테두리까지도 조금 더 정확하게 평가 가능.
L1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding='SAME')
L1 = tf.nn.relu(L1) # relu 활성화 함수통해 컨볼루션 계층을 완성

# 풀링계층 만들기.
# 이 코드는 앞서 만든 컨볼루션 계층을 입력층으로 사용하고 커널 크기를 2x2로 하는 풀링계층 만듬
# strides=[1,2,2,1] 값은 슬라이딩시 두칸씩 움직이겠다는 옵션
L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')

# 3x3 크기의 컨널 64개로 구성한 컨볼루션 계층과 2x2 크기의 풀링계층으로 구성
# 두번째 컨볼루션 계층의 커널인 W2 변수의 구성은 [3,3,32,64]. 여기에서 32는 앞서 구성한 첫번쨰 계층의 커널개수. 이것은 출력층의 개수이며 첫번째 컨볼루션 계층이 찾아낸 이미지의 특징 개수.
W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))
L2 = tf.nn.conv2d(L1, W2, strides=[1,1,1,1], padding='SAME')
L2 = tf.nn.relu(L2)
L2 = tf.nn.max_pool(L2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')


# 이제 추출한 특징들을 이용해 10개의 분류를 만들어내는 계층 구성
# 먼저 10개의 분류는 1차원 배열이므로 차원을 줄이는 단계를 거쳐야 함.
# 직전의 풀링 계층의 크기가 7x7x64 이므로, 먼저 tf.shape 함수 이용해 7x7x64 크기의 1차원 계층으로 만들고
# 이 배열 전체를 최종 출력값의 중간단계인 256개의 뉴런으로 연결하는 신경망 만들어줌. 
# 참고로 이처럼 인접한 계층의 모든 뉴런과 상호연결된 계층을 '완전연결계층' 이라 함
# 이번계층에는 과적합을 막아주는 드롭아웃 기법 사용
W3 = tf.Variable(tf.random_normal([7 * 7 * 64, 256], stddev=0.01))
L3 = tf.reshape(L2, [-1, 7 * 7 * 64])
L3 = tf.matmul(L3, W3)
L3 = tf.nn.relu(L3)
L3 = tf.nn.dropout(L3, keep_prob)

# 모델 구성의 마지막으로 직전 은닉층인 L3의 출력값을 256개를 받아 최종 출력값인 0~9 레이블을 갖은 10개의 출력값을 만듬
W4 = tf.Variable(tf.random_normal([256,10], stddev=0.010))
model = tf.matmul(L3, W4)

# 손실함수와 AdamOptimizer를 이용한 최적화 함수 만듬
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))
optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)
# optimizer = tf.train.RMSProOptimizer(0.001, 0.9).minimize(cost) 사용가능


# 학습시키고 결과를 확인하는 코드 작성
# 앞에서 구성한 신경망 모델을 초기화하고 학습을 진행할 세션을 시작
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

# MNIST 는 데이터수가 수만개로 매우 크므로 미니배치 방식으로 학습
batch_size = 100      # 미니배치의 크기는 100개
total_batch = int(mnist.train.num_examples / batch_size)    
# mnist.train.num_examples를 배치크기로 나눠 미니배치가 총 몇개인지를 저장해둠

# MNIST 데이터 전체를 학습하는 일을 총 15번 반복 ( 학습데이터 전체를 한바퀴 도는 것 = epoch )
for epoch in range(15):
    total_cost = 0

# mnist.train.next_batch -> 학습할 데이터를 배치크기만큼 가져온뒤, 입력값인 이미지 데이터는 batch_xs에
# 출력값인 레이블 데이터는 batch_ys에 저장.
    for i in range(total_batch):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
        # 모델에 입력값 전달하기 위해 MNIST 데이터를 28x28형태로 재구성
        batch_xs = batch_xs.reshape(-1, 28,28, 1)

# sess.run을 이용해 최적화 시키고 손실값을 가져와 저장
# 이때 feed_dict 매개변수에 입력값 X와 예측을 평가할 실제 레이블값 Y에 사용할 데이터를 넣어줌.
        _, cost_val = sess.run([optimizer, cost],
                               feed_dict = {X: batch_xs, Y: batch_ys,
                                            keep_prob:0.7})
# 그리고 손실값을 저장한후, 한세대의 학습이 끝나면 학습한 세대의 평균 손실값을 출력함.
        total_cost += cost_val

    print('Epoch:', '%04d' % (epoch + 1),
          'Avg. cost = ', '{:.3f}'.format(total_cost / total_batch))

print('최적화 완료')


# 예측결과인 model의 값과 실제 레이블인 Y의 값을 비교
# 예측한 결과값은 원-핫 인코딩 형식이며 각 인덱스에 해당하는 값은 다음처럼 해당숫자가 얼마나 해당인덱스와 관련이 높은가를 나타냄. <- 손실값을 softmax_cross_entropy_with_logits를 이용해 구했기 떄문.
# tf.argmax(model, 1) 은 두번째 차원(1번 인덱스의 차원)의 값 중 최댓값의 인덱스를 뽑아내는 함수.
# model로 출력한 결과는 [None, 10]처럼 결과값을 배치 크기만큼 가지고 있음
is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))

# tf.cast 이용해 is_correct를 0과 1로 변환 그리고 변환한 값들을 tf.reduce_mean을 이요해 평균을 내면 그것이 바로 정확도(확률)이 됨
accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))

# 테스트 데이터를 다루는 객체인 mnist.test 를 이용해 테스트 이미지와 레이블 데이터를 넣어 accuracy를 계산
# 모델에 입력값 전달하기 위해 MNIST 데이터를 28x28형태로 재구성
print('정확도: ', sess.run(accuracy,
                        feed_dict={X: mnist.test.images.reshape(-1,28,28,1),
                                   Y: mnist.test.labels, keep_prob: 1}))



'''고수준 API'''
# tf.layers 등의 고급 API를 사용하면 활성화 함수나 컨볼루션 계층을 만들기 위한 나머지 수치들을 알아서 계산하고 적용함. 또한 가중치를 초기화 할때 기본적으로 xavier_initializer 함수를 사용하는 등 신경망을 효율적으로 만들어 주는 옵션들도 쉽계 사용할수 있도록 해줌


#첫번째 컨볼루션 및 풀링 계층
W1 = tf.Variable(tf.random_normal([3,3,1,32], stddev=0.01))
L1 = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='SAME')
L1 = tf.nn.relu(L1)
L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')

# 위 코드와 같음 tf.layers 모듈 사용하여 간편하고 짧아짐
L1 = tf.layers.conv2d(X, 32, [3,3])
L1 = tf.layers.max_pooling2d(L1, [2,2], [2,2])


# 완전연결계층을 만드는 코드
W3 = tf.Variable(tf.random_normal([7*7*64, 256], stddev=0.01))
L3 = tf.reshape(L2, [-1, 7*7*64])
L3 = tf.matmul(L3, W3)
L3 = tf.nn.relu(L3)

# 위 코드와 같음 tf.layers 모듈 사용하여 간편하고 짧아짐
L3 = tf.contrib.layers.flatten(L2)
L3 = tf.layers.dense(L3, 256, activation=tf.nn.relu)





'''비지도 학습 : Autoencoder'''

''' 
오토인코더는 입력값과 출력값을 같게하는 신경망. 가운데 계층의 노드수가 입력값보다 적은것이 특징
이런 구조로 인해 데이터를 압축하는 효과를 얻고, 이 과정이 노이즈 제거에 매우 효과적임
오토인코더의 핵심은 입력층으로 들어온 데이터를 인코더를 통해 은닉층으로 내보내고, 
은닉층의 데이터를 디코더를 통해 출력층으로 내보낸 뒤, 만들어진 출력값을 입력값과 비슷해지도록 
가중치를 찾아내는 것
'''

import numpy as np
import matplotlib.pyplot as plt

mnist = input_data.read_data_sets('./mnist/data/', one_hot=True)

# 하이퍼파라미터로 사용할 옵션들을 따로 빼내어 코드를 구조화
learning_rate = 0.01
training_epoch = 20
batch_size = 100
n_hidden = 256      # 은닉층 뉴런의 개수
n_input = 28*28     # 입력값의 크기로 MNIST의 이미지 크기가 28x28이므로 784

# 먼저 X의 플레이스홀더 설정. 이모델은 비지도 학습이므로 Y값이 없음
X = tf.placeholder(tf.float32, [None, n_input])

# 오토인코더의 핵심 모델은 인코더와 디코더를 만드는것
# 인코더와 디코더를 만드는 방식에 따라 다양한 오토인코더를 만들수 있음
# 인코더 먼저 만들기
# 맨처음 n_hidden 개의 뉴런을 가진 은닉층 만듬. 가중치와 편향 변수를 원하는 뉴런 개수만큼 설정하고
# 그 변수들을 입력값과 곱하고 더한 뒤 활성화 함수인 sigmoid 함수를 적용
# 이떄 중요한 것은 n_input 값보다 n_hidden 값이 더 작다는 것. 이렇게 하면 입력값을 압축하고 노이즈를
# 제거하면서 입력값의 특징을 찾아냄
W_encode = tf.Variable(tf.random_normal([n_input, n_hidden]))
b_encode = tf.Variable(tf.random_normal([n_hidden]))

encoder = tf.nn.sigmoid(
    tf.add(tf.matmul(X, W_encode), b_encode))


# 디코더 만들기
# 입력값을 은닉층의 크기로, 출력값을 입력층의 크기로 만듬.
W_decode = tf.Variable(tf.random_normal([n_hidden, n_input]))
b_decode = tf.Variable(tf.random_normal([n_input]))

decoder = tf.nn.sigmoid(
    tf.add(tf.matmul(encoder, W_decode), b_decode))


# 그런후 가중치들을 최적화하기위한 손실함수 만듬. 우리가 만드는 기본적인 오토인코더의 목적은 출력값을 입력값과 가장 비슷하게 만드는 것. 그렇게 하면 압축된 은닉층의 뉴런들을 통해 입력값의 특징을 알아낼수 있음. 따라서 입력값인 X를 평가하기 위한 실측값으로 사용하고 디코더가 내보낸 결과값과의 차이를 손실값으로 설정. 그리고 이 값의 차이는 거리함수로 구함.
cost = tf.reduce_mean(tf.pow(X - decoder, 2))


# RMSPropOptimizer 함수를 이용한 최적화 함수 설정
optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)


# 학습시키고 결과를 확인하는 코드 작성
# 앞에서 구성한 신경망 모델을 초기화하고 학습을 진행할 세션을 시작
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)


total_batch = int(mnist.train.num_examples / batch_size)
# mnist.train.num_examples를 배치크기로 나눠 미니배치가 총 몇개인지를 저장해둠

for epoch in range(training_epoch):
    total_cost = 0

# mnist.train.next_batch -> 학습할 데이터를 배치크기만큼 가져온뒤, 입력값인 이미지 데이터는 batch_xs에
# 출력값인 레이블 데이터는 batch_ys에 저장.
    for i in range(total_batch):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
# sess.run을 이용해 최적화 시키고 손실값을 가져와 저장
# 이때 feed_dict 매개변수에 입력값 X에 사용할 데이터를 넣어줌.
        _, cost_val = sess.run([optimizer, cost],
                               feed_dict = {X: batch_xs})
# 그리고 손실값을 저장한후, 한세대의 학습이 끝나면 학습한 세대의 평균 손실값을 출력함.
        total_cost += cost_val

    print('Epoch:', '%04d' % (epoch + 1),
          'Avg. cost = ', '{:.4f}'.format(total_cost / total_batch))

print('최적화 완료')


# 이번에는 결과값을 정확도가 아닌 디코더로 생성해낸 결과를 직관적인 방법으로 확이해보기
# 결과를 확인하는 방법은 다양하지만 여기서는 matplotlib을 이용해 이미지로 출력
# 먼저 총 10개의 테스트 데이터를 가져와 디코더를 이용해 출력값으로 만듬
sample_size = 10
sample = sess.run(decoder,
                  feed_dict={X: mnist.test.images[:sample_size]})

# 그런 다음 numpy 모듈을 이용해 MNIST 데이터를 28x28 크기의 이미지 데이터로 재구성한뒤,
# matplotlib의 imshow 함수이용해 그래프에 이미지로 출력.
# 위쪽에는 입력값의 이미지를 , 아래쪽에는 신경망으로 생성한 이미지 출력
fig, ax = plt.subplot(2, sample_size, figsize=(sample_size, 2))

for i in range(sample_size):
    ax[0][i].set_axis_off()
    ax[1][i].set_axis_off()
    ax[0][i].imshow(np.reshape(mnist.test.images[i], (28,28)))
    ax[1][i].imshow(np.reshape(sample[i], (28,28)))

plt.show()




''' GAN '''

'''
결과물을 생성하는 모델로 서로 대립하는 두 신경망을 경쟁시켜가며 결과물 생성방법 학습
ex) 위조지폐범(생성자)가 경찰을 최대한 속이려고 노력하고 경찰(구분자)은 위조한 지폐를 최대한 감별하려고 노력. 이처럼 위조지폐를 만들고 감별 하려는 경쟁을 통해 서로의 능력이 발전. 거의 구분할수 없을 정도로 진짜 같은 위조지폐를 만들수 있음.
실제 이미지를 주고 구분자에게 이 이미지가 진짜임을 판단하게 함. 그런 다음 생성자를 통해 노이즈로부터 임의의 이미지를 만들고 이것을 다시 같은 구분자를 통해 진짜 이미지인지 판단. 이렇게 생성자는 구분자를 속여 진짜처럼 보이게 하고, 구분자는 생성자가 만든 이미지를 최대한 가짜라고 구분하도록 훈련 하는것이 GAN의 핵심. 이렇게 생성자와 구분자의 경쟁을 통해 결과적으로 생성자는 실제 이미지와 상당히 비슷한 이미지를 생성해 낼수 있음.
이미지 생성 외에도 자연어 문장 생성등에 관한 연구도 진행중.  
'''

# GAN 모델을 이용해 MNIST 손글씨 숫자를 무작위로 생성하는 예제 만들기
#  모델을 확장하여 원하는 숫자에 해당하는 이미지를 생성하는 모델 만들기
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('./mnist/data/', one_hot=True)


# 하이퍼파라미터로 사용할 옵션들을 따로 빼내어 코드를 구조화
total_epoch = 100
learning_rate = 0.0002
batch_size = 100
n_hidden = 256      # 은닉층 뉴런의 개수
n_input = 28*28     # 입력값의 크기로 MNIST의 이미지 크기가 28x28 이므로 784
n_noise = 128       # 생성자의 입력값으로 사용할 노이즈의 크기. 랜덤한 노이즈를 입력하고 그 노이즈에서
                    # 손글씨 이미지를 무작위로 생성하도록 함.

# 플레이스홀드 설정. GAN도 비지도학습이므로 Y를 사용하지 않음
# 다만 구분자에 넣을 이미지가 실제 이미지와 생성한 가짜 이미지 두개. 가짜 이미지는 노이즈에서 생성할 것이므로 노이즈를 입력할 플레이스홀더 Z 추가.
X = tf.placeholder(tf.float32, [None, n_input])
Z = tf.placeholder(tf.float32, [None, n_noise])

# 생성자 신경망에 사용할 변수들을 설정. 첫번째 가중치와 편향은 은닉층으로 출력하기 위한 변수들
# 두번째 가중치와 편향은 출력층에 사용할 변수들. 따라서 두번째 가중치의 변수크기는 실제 이미지의 크기와
# 같아야함. 그 크기가 바로 n_input으로 28x28인 784가 됨.
G_W1 = tf.Variable(tf.random_normal([n_noise, n_hidden], stddev=0.01))
G_b1 = tf.Variable(tf.zeros([n_hidden]))
G_W2 = tf.Variable(tf.random_normal([n_hidden, n_input], stddev=0.01))
G_b2 = tf.Variable(tf.zeros([n_input]))

# 이어서 구분자 신경망에 사용할 변수들 설정. 은닉층은 생성자와 동일하게 구성.
# 구분자는 진짜와 얼마나 가까운가를 판단하는 값으로, 0~1 사이의 값을 출력.
# 따라서 하나의 스칼라 값을 출력하도록 구성.
D_W1 = tf.Variable(tf.random_normal([n_input, n_hidden], stddev=0.01))
D_b1 = tf.Variable(tf.zeros([n_hidden]))
D_W2 = tf.Variable(tf.random_normal([n_hidden, 1], stddev=0.01))
D_b2 = tf.Variable(tf.zeros([1]))

# 실제 이미지를 판별하는 구분자 신경망과 생성한 이미지를 판별하는 구분자 신경망은 같은 변수를 사용해야함.
# 같은 신경망으로 구분을 시켜야 진짜 이미지와 가짜 이미지를 구분하는 특징들을 동시에 잡아낼수 있음.

# 이제 생성자와 구분자 신경망을 구성
# 먼저 생성자 신경망
# 생성자 무작위로 생성한 노이즈를 받아 가중치와 편향을 반영하여 은닉층을 만들고 은닉층에서 실제 이미지와
# 같은 크기의 결괏값을 출력하는 간단한 구성.
def generator(noise_z):
    hidden = tf.nn.relu(
        tf.matmul(noise_z, G_W1) + G_b1)
    output = tf.nn.sigmoid(
        tf.matmul(hidden, G_W2) + G_b2)
    return output

# 구분자 신경망은 0~1 사이의 스칼라값 하나를 출력하도록 하였으며, 이를 위한 활성화 함수로 sigmoid함수 사용
def discriminator(inputs):
    hidden = tf.nn.relu(
        tf.matmul(inputs, D_W1) + D_b1)
    output = tf.nn.sigmoid(
        tf.matmul(hidden, D_W2) + D_b2)
    return output

# 무작위한 노이즈를 만들어주는 함수 만들어줌
def get_noise(batch_size, n_noise):
    return np.random.normal(size=(batch_size, n_noise))

# 노이즈 Z를 이용해 가짜 이미지를 만들 생성자 G를 만들고, 이 G가 만든 가짜 이미지와 진짜 이미지 X를
# 각각 구분자에 넣어 입력한 이미지가 진짜인지를 판별하도록 함.
# 이들을 잘 학습시키면 생성자가 실제에 가까운 이미지를 만들수 있게 함.
G = generator(Z)
D_gene = discriminator(G)
D_real = discriminator(X)

# 다음으로는 손실값을 구해야 하는데 이번에는 두개가 필요. 즉, 생성자가 만든 이미지를 구분자가 가짜라고 판단하도록 하는 손실값 (경찰 학습용) 과 진짜라고 판단하도록 하는 손실값 (위조 지폐범 학습용)을 구해야하 함.
# 경찰을 학습시키려면 진짜 이미지 판별값 D_real은 1에 가까워야 하고 (진짜라고 판별), 가짜이미지 판별값 D_gene는 0에 가까워야 함.(가짜라고 판별)
loss_D = tf.reduce_mean(tf.log(D_real) + tf.log(1 - D_gene))

# 간단하게 설명해 'D_real'과 '1에서 D_gene을 뺀 값'을 더한 값을 손실값으로 하며 이 값을 최대화 하면 경찰학습이 이루어짐
# 다음으로 위조지폐범 학습은 가짜 이미지 판별값 D_gene을 1에 가깝게 만들기만 하면 됨.
# 즉, 가짜 이미지를 넣어도 진짜 같다고 판별. 다음과 같이 D_gene을 그대로 넣어 이를 손실값으로 하고, 이값을 최대화하면 위조 지폐범을 학습시킬수 있음.
loss_G = tf.reduce_mean(tf.log(D_gene))

# 즉, GAN의 학습은 loss_D와 loss_G 모두를 최대화 하는 것. 다만 loss_D와 loss_G는 서로 연관되어 있어
# 두 손실값이 항상 같이 증가하는 경향을 보이지는 않음. loss_D가 증가하려면 loss_G는 하락해야 하고
# loss_D는 하락해야 하는 경쟁 관계기 때문.


# 이제 이 손실값들을 이용해 학습시키는 일만 남았음. 이때 주의할 것이 하나 있음.
# loss_D를 구할때는 구분자 신경망에 사용되는 변수들만 사용하고, loss_G를 구할 때는 생성자 신경망에 사용되는 변수들만 사용하여 최적화해하함.
# 그래야 loss_D를 학습할 때는 생성자가 변하지 않고, loss_G를 학습할 때 구분자가 변하지 않기 때문
D_var_list = [D_W1, D_b1, D_W2, D_b2]
G_var_list = [G_W1, G_b1, G_W2, G_b2]

# 이어서 변수들을 최적화하는 함수들을 구성. GAN 논문에 따르면 loss를 최대화 해야 하지만 최적화에 쓸 수있는 함수는 mininmize 뿐이므로 최적화하려면 loss_D 와 loss_G에 음수부호를 붙여줌
train_D = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D, var_list=D_var_list)
train_G = tf.train.AdamOptimizer(learning_rate).minimize(-loss_G, var_list=G_var_list)


# 이것으로 준비는 끝나고 학습을 시키는 코드 작성
# GAN 모델은 두개의 손실값을 학습시켜야함
sess = tf.Session()
sess.run(tf.global_variables_initializer())

total_batch = int(mnist.train.num_examples / batch_size)
loss_val_D, loss_val_G = 0, 0       # loss_D와 loss_G의 결과값을 받을 변수 설정

# 그런 후 미니배치로 반복 학습. 여기서 구분자는 X 값을, 생성자는 노이즈인 Z 값을 받으므로 노이즈를 생성해주는 get_noise 함수를 통해 배치 크기만큼 노이즈를 만들고 이를 입력해줌.
# 그리고 구분자와 생성자 신경망을 각각 학습시킴
for epoch in range(total_batch):
    for i in range(total_batch):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
        noise = get_noise(batch_size, n_noise)

        _, loss_val_D = sess.run([train_D, loss_D],
                                 feed_dict={X: batch_xs, Z: noise})
        _, loss_val_G = sess.run([train_G, loss_G],
                                 feed_dict={Z: noise})
    print('Epoch: ', '%04d' % epoch,
          'D loss: {:.4}'.format(loss_val_D),
          'G loss: {:.4}'.format(loss_val_G))

# GAN 모델을 완성하였으니 학습결과를 확인하는 코드 작성
# 학습이 잘되는지 0번째, 9번째, 19번쨰,29번쨰.. 마다 생성기로 이미지를 생성하여 눈으로 직접 확인함.
# 결과를 확인하는 코드는 학습 루프안에 작성 노이즈를 만들고 이것을 생성자 G에 넣어 결과값 만든뒤
if epoch == 0 or (epoch + 1) % 10 == 0:
    sample_size = 10
    noise = get_noise(sample_size, n_noise)
    samples = sess.run(G, feed_dict={Z: noise})
    #이 결과값들을 28x28 크기의 가짜 이미지로 만들어 samples 폴더에 저장. (samples 폴더는 미리 만들어져있어야함)
    fig, ax = plt.subplots(1, sample_size, figsize=(sample_size, 1))

    for i in range(sample_size):
        ax[i].set_axes_off()
        ax[i].imshow(np.reshape(samples[i], (28, 28)))

    plt.savefig('samples/{}.png'.format(str(epoch).zfill(3)),
                bbox_inches='tight')
    plt.close(fig)

print('최적화 완료!')

# 실행결과 : 노이즈가 무작위로 만들어지니 매번 조금씩 다른 이미지가 생성됨.
# 중요한 점은 여기서 생성된 이미지들은 MNIST 데이터에 없는 새로운 손글씨 이미지 라는 것
