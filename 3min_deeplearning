import tensorflow as tf

# tf.constant로 상수를 hello 변수에 저장
hello = tf.constant('Hello, TensorFlow!')
print(hello)
# 결과 :
# Tensor("Const:0", shape=(), dtype=string)

# hello가 텐서플로의 텐서라는 자료형이고 상수를 담고 있음

# 랭크가 0인 텐서; 셰이프는 []

[1., 2., 3.]    # 랭크가 1인 텐서; 셰이프는 [3]
[[1., 2., 3.], [4., 5., 6.]]    # 랭크가 2인 텐서; 셰이프는 [2, 3]
[[[1., 2., 3.]], [[7., 8., 9]]] # 랭크가 3인 텐서; 셰이프는 [2, 1, 3]

# 랭크는 차원의 수를 의미
# 랭크가 0이면 스칼라, 1이면 벡터, 2면 행렬, 3이상이면 n-Tensor 또는 n차원 텐서라고 부름
# 셰이프는 각 차원의 요소 개수로 텐서의 구조를 설명해줌

# 텐서를 출력할때 나오는 dtype은 해당 텐서에 담긴 요소들의 자료형 - string, float, int 등..

# 텐서를 이용해 다양한 연산 가능
a = tf.constant(10)
b = tf.constant(32)
c = tf.add(a,b)

print(c)


# 그래프는 텐서들의 연산 모음
# 텐서플로는 지연실행 방식
# : 텐서플로는 텐서와 텐서의 연산들을 먼저 정의하여 그래프를 만들고 이후 필요한 연산을
# 실행하는 코드를 넣어 원하는 시점에 실제 연산을 수행하도록 함.

#그래프의 실행은 Session() 안에서 이루어 져야 하며 다음과 같이 Session 객체와 run 메서드 사용
sess = tf.Session()
print(sess.run(hello))
print(sess.run([a,b,c]))

sess.close()




# 플레이스홀더
# : 그래프에 사용할 입력값을 나중에 받기 위해 사용하는 매개변수

# 변수
# : 그래프를 최적화하는 용도로 텐서플로가 학습한 결과를 갱신하기 위해 사용하는 변수
# 이 변수의 값들이 신경망의 성능을 좌우함

X = tf.placeholder(tf.float32, [None, 3])   # None은 크기가 정해지지 않음을 의미
print(X)

# 앞서 텐서의 모양을 (?, 3)으로 정의했으므로 두번째 차원은 요소를 3개씩 가지고 있어야 함.
x_data = [[1, 2, 3], [4, 5, 6]]

# W와 b에 각각 텐서플로의 변수를 생성하여 할당
# W는 [3,2] 행렬 형태의 텐서, b는 [2,1] 행렬 형태의 텐서로 tf.random_normal 함수를 이용해 정규 분포 형태의 무작위 값으로 초기화
W = tf.Variable(tf.random_normal([3, 2]))
b = tf.Variable(tf.random_normal([2, 1]))

W = tf.Variable([[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]])

expr = tf.matmul(X, W) + b

# 이제 연산을 실행하고 결과를 출력하여 설정한 텐서들과 계산된 그래프의 결과를 확인
sess = tf.Session()

# 앞에서 정의한 변수들을 초기화 하는 함수
# 기존에 학습한 값들을 가져와서 사용하는 것이 아닌 처음 실행하는 것이라면 연산을 실행하기전에
# 반드시 이 함수를 이용해 변수들을 초기화 해야 함.
sess.run(tf.global_variables_initializer())

print('=== x_data ===')
print(x_data)
print('=== W ===')
print(sess.run(W))
print('=== b ===')
print(sess.run(b))
print('=== expr ===')
print(sess.run(expr, feed_dict={X: x_data})) # feed_dict 매개변수는 그래프를 실행할 때 사용할 입력값을 지정.
# expr 수식에는 X, W, b를 사용했는데 이중 X가 플레이스홀더라 X에 값을 넣어주지 않으면 계산에 사용할 값이 없으므로 에러발생. 따라서 미리 정의 해둔 x_data를 X의 값으로 넣어줌

sess.close()




''' 선형회귀 모델 구현하기'''

x_data = [1,2,3]
y_data = [1,2,3]

W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))
b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))

X = tf.placeholder(tf.float32, name='X')
Y = tf.placeholder(tf.float32, name='Y')

hypothesis = W * X + b

# 손실함수, 비용함수
# : 한 쌍(x, y)의 데이터에 대한 손실값을 계산하는 함수
# 손실값이란 실제값과 모델로 예측한 값이 얼마나 차이가 나타내는 값
# 즉 손실값이 작을수록 그 모델이 X와  Y의 관계를 잘 설명하고 있다는 뜻.
# 이 손실을 전체 데이터에 대해 구한 경우 이를 비용(cost) or 손실(loss)라고 함.
cost = tf.reduce_mean(tf.square(hypothesis - Y))


# 경사하강법 최적화 함수를 이용해 손실값을 최소화하는 연산 그래프 생성
# 최적화 함수란 가중치와 편향값을 변경해가면서 손실값을 최소화하는 가장 최적화된 가중치와 편향 값을 찾아주는 함수. 이때 무작위로 값들을 변경하면 시간이 너무 오래걸리므로 빠르게 최적화 하기 위한 즉, 빠르게 학습하기 위한 다양한 방법 사용.  ex) 경사하강법

# 최적화 함수의 매개변수인 learning_rate (학습률) 은 학습을 얼마나 급하게 할것인가를 설정하는 값.  - 하이퍼파라미터중 하나
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
train_op = optimizer.minimize(cost)


# 이제 선형회귀 모델을 다 만들었으니 그래프를 실행해 학습시키고 결과를 확인
# 파이썬의 with 기능 이용해 세션블록을 만들고 세션을 자동 종료로 처리하도록 만듬
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

# 최적화를 수행하는 그래프인 train_op를 실행하고 실행시마다 변환하는 손실값을 출력하는 코드
# 학습은 100번 수행, feed_dict 매개변수를 통해, 상관관계를 알아내고자하는 데이턴인
# x_data와 y_data를 입력해줌.
    for step in range(100):
        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data,
                                                            Y: y_data})
        print(step, cost_val, sess.run(W), sess.run(b))

    # 학습에 사용되지 않았던 값인 5와 2.5를 X값으로 넣고 결과를 확인
    print("X: 5, Y: ", sess.run(hypothesis, feed_dict={X: 5}))
    print("X: 2.5, Y: ", sess.run(hypothesis, feed_dict={X: 2.5}))




''' 분류 모델 구현하기 '''
import numpy as np

# 학습에 사용할 데이터 정의
x_data = np.array(
    [[0,0], [1,0], [1, 1], [0, 0], [0, 0], [0, 1]])

# 레이블 데이터는 원-핫 인코딩으로 구성함
# y_data 를 원핫 인코딩 형태로 만듬
y_data = np.array([
    [1, 0, 0],
    [0, 1, 0],
    [0, 0, 1],
    [1, 0, 0],
    [1, 0, 0],
    [0, 0, 1]
])

X = tf.placeholder(tf.float32)
Y = tf.placeholder(tf.float32)

W = tf.Variable(tf.random_uniform([2, 3], -1., 1.))
b = tf.Variable(tf.zeros([3]))

L = tf.add(tf.matmul(X, W), b)
L = tf.nn.relu(L)

model = tf.nn.softmax(L)

# 손실함수는 원-핫 인코딩을 이용한 대부분의 모델에서 교차 엔트로피 함수 사용
# 교차 엔트로피 값은 예측값과 실제 값 사이의 확률 분포 차이를 계산한 값
cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(model), axis=1))

# 경사하강법으로 초기화
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train_op = optimizer.minimize(cost)

# 텐서플로의 세션 초기화
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

# 앞서 구성한 특징과 레이블 데이터를 이용해 학습을 100번 진행
for step in range(100):
    sess.run(train_op, feed_dict={X: x_data, Y: y_data})

    # 학습 도중 10번에 한번씩 손실값을 출력
    if (step + 1) % 10 == 0:
        print(step + 1, sess.run(cost, feed_dict={X: x_data, Y: y_data}))

# 학습된 결과를 확인하는 코드
prediction = tf.argmax(model, axis=1)
target = tf.argmax(Y, axis=1)
print('예측값: ', sess.run(prediction, feed_dict={X: x_data}))
print('실제값: ', sess.run(target, feed_dict={Y: y_data}))


is_correct = tf.equal(prediction, target)

accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))

W1 = tf.Variable(tf.random_uniform([2, 10], -1., 1.))
W2 = tf.Variable(tf.random_uniform([10, 3], -1., 1.))

b1 = tf.Variable(tf.zeros([10]))
b2 = tf.Variable(tf.zeros([3]))

# 가중치
W1 = [2, 10]   # [특징, 은닉층의 뉴련수]
W2 = [10, 3]   # [은닉층의 뉴런수, 분류수]

# 편향
b1 = [10]
b2 = [3]

# L1 = tf.add(tf.matmul(X, W1), b1)
# L1 = tf.nn.relu(L1)
#
# model = tf.add(tf.matmul(L1, W2), b2)
#
# cost = tf.reduce_mean(
#     tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=model))
#
# optimizer = tf.train.AdamOptimizer(learning_rate=0.01)
# train_op = optimizer.minimize(cost)
#






'''텐서보드와 모델 재사용'''
data = np.loadtxt('./data.csv', delimiter=',',
                  unpack=True, dtype='float32')
print(data)
x_data = np.transpose(data[0:2])
y_data = np.transpose(data[2:])
print("x_data\n",x_data)
print(y_data)

# 모델을 저장할때 쓸 변수 하나 만들기
# 이 변수는 직접 학습에 사용되지 않고 학습을 카운트 하는 변수. <= trainable=False
global_step = tf.Variable(0, trainable=False, name='global_step')

#
X = tf.placeholder(tf.float32)
Y = tf.placeholder(tf.float32)

W1 = tf.Variable(tf.random_uniform([2, 10], -1., 1.))
L1 = tf.nn.relu(tf.matmul(X, W1))

# W2의 shape 는 앞단 계층의 출력크기가 10이고 뒷단 계층의 입력크기가 20이기 때문 = [10,20]
W2 = tf.Variable(tf.random_uniform([10, 20], -1., 1.))
L2 = tf.nn.relu(tf.matmul(L1, W2))

W3 = tf.Variable(tf.random_uniform([20, 3], -1., 1.))
model = tf.matmul(L2, W3)

cost = tf.reduce_mean(
    tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=model))

optimizer = tf.train.AdamOptimizer(learning_rate=0.01)
# global_step 매개변수에 앞서 정의한 global_step 변수를 넘겨줌. 이렇게 하면 최적화 함수가
# 학습용 변수들을 최적화 할때마다 global_step 변수의 값을 1씩 증가 시킴
train_op = optimizer.minimize(cost, global_step=global_step)


# 모델 구성이 모두 끝났으니 이제 세션을 열고 최적화 실행하기
sess = tf.Session()
saver = tf.train.Saver(tf.global_variables())

# ./model 디렉터리에 기존에 학습해둔 모델이 있는 지를 확인해서 모델이 있다면 saver.restore 함수를 사용해 학습된 값들을 불러오고, 아니면 변수를 새로 초기화함.
# 학습된 모델을 저장한 파일을 체크포인트 파일 이라고 함.
ckpt = tf.train.get_checkpoint_state('./model')
if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
    saver.restore(sess, ckpt.model_checkpoint_path)
else:
    sess.run(tf.global_variables_initializer())

for step in range(2):
    sess.run(train_op, feed_dict={X: x_data, Y: y_data})

    print('Step: %d, ' % sess.run(global_step),
          'Cost: %.3f' % sess.run(cost, feed_dict={X: x_data, Y: y_data}))

saver.save(sess, './model/dnn.ckpt', global_step=global_step)

prediction = tf.argmax(model, 1)
target = tf.argmax(Y, 1)
print('예측값:', sess.run(prediction, feed_dict={X: x_data}))
print('실제값:', sess.run(target, feed_dict={Y: y_data}))

is_correct = tf.equal(prediction, target)
accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data,
                                                           Y: y_data}))


'''텐서보드 사용하기'''

# 딥러닝 학습시간이 김 => 모델을 효과적으로 실험하려면 학습과정을 추적하는 일이 매우중요
# => 이런 어려움을 해결해주고자 텐서플로는 텐서보드라는 도구를 기본으로 제공
# 텐서보드는 학습하는 중간중간 손실값이나 정확도 또는 결과물이 나온 이미지나 사운드파일들을 다양한 방식으로 시각화해 보여줌.

import tensorflow as tf
import numpy as np

data = np.loadtxt('./data.csv', delimiter=',',
                  unpack=True, dtype='float32')

x_data = np.transpose(data[0:2])
y_data = np.transpose(data[2:])

global_step = tf.Variable(0, trainable=False, name='global_step')

X = tf.placeholder(tf.float32)
Y = tf.placeholder(tf.float32)


with tf.name_scope('layer1'):
    W1 = tf.Variable(tf.random_uniform([2, 10], -1., 1.), name='W1')
    L1 = tf.nn.relu(tf.matmul(X, W1))

with tf.name_scope('layer2'):
    W2 = tf.Variable(tf.random_uniform([10, 20], -1., 1.), name='W2')
    L2 = tf.nn.relu(tf.matmul(L1, W2))

with tf.name_scope('layer1'):
    W3 = tf.Variable(tf.random_uniform([20, 3], -1., 1.), name='W3')
    model = tf.matmul(L2, W3)

with tf.name_scope('optimizer'):
    cost = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=model))

    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)
    train_op = optimizer.minimize(cost, global_step=global_step)

# 손실값을 추적하기위해 수집할 값을 지정하는 코드 작성
# tf.summary 모듈의  scalar 함수는 값이 하나인 텐서를 수집할때 사용.
    tf.summary.scalar('cost', cost)
tf.global_variables_initializer()
sess = tf.Session()

saver = tf.train.Saver(tf.global_variables())

ckpt = tf.train.get_checkpoint_state('./model')
if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
    saver.restore(sess, ckpt.model_checkpoint_path)
else:
    sess.run(tf.global_variables_initializer())

# tf.summary.merge_all 함수로 앞서 지정한 텐서들을 수집
# tf.summary.FileWriter 함수를 이용해 그래프와 텐서들의 값을 저장할 디렉터리 설정
merged = tf.summary.merge_all()
writer = tf.summary.FileWriter('./logs', sess.graph)

# 최적화 실행하는 코드
for step in range(100):
    sess.run(train_op, feed_dict={X: x_data, Y: y_data})

    print('Step: %d, ' % sess.run(global_step),
          'Cost: %.3f' % sess.run(cost, feed_dict={X: x_data, Y: y_data}))

# sess.run을 이용해 앞서 merged로 모아둔 텐서의 값들을 계산하여 수집한뒤,
# writer.add_summary 함수를 이용해 해당 값들을 앞서 지정한 디렉터리에 저장
# 나중에 확인할수 있도록 global_step 값을 이용해 수집한 시점을 기록해 둠.
    summary = sess.run(merged, feed_dict={X: x_data, Y: y_data})
    writer.add_summary(summary, global_step=sess.run(global_step))


saver.save(sess, './model/dnn.ckpt', global_step=global_step)

prediction = tf.argmax(model, 1)
target = tf.argmax(Y, 1)
print('예측값: ', sess.run(prediction, feed_dict={X: x_data}))
print('실제값: ', sess.run(target, feed_dict={Y: y_data}))

is_correct = tf.equal(prediction, target)
accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
print('정확도: %.2f'  % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))





''' MNIST '''

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('./mnist/data/', one_hot=True)


# MNIST의 손글씨 이미지는 28x28 픽셀로 이루어짐. 즉, 784개의 특징으로 이루어져 있음.
# 그리고 레이블은 0부터 9까지이므로 10개의 분류로 나눔
# 입력과 출력인 X와 Y를 다음과 같이 정의함.

# 이미지를 하나씩 학습시키는것보다 여러개를 한꺼번에 학습시키는 쪽이 효과가 좋지만 그만큼 많은 메모리와
# 높은 컴퓨팅 성능이 뒷받침 되야함 => 데이터를 적당한 크기로 잘라서 학습 시킴 : 미니배치
# X와 Y코드에서 텐서의 첫번째 차원이 None으로 되어 있는 것을 볼수 있는데 이 자리에는 한번에 학습시킬
# MNIST 이미지의 개수를 지정하는 값이 들어감.
# 원하는 배치 크기로 정확히 명시해줘도 되지만 한번에 학습할 개수를 계속 바꿔가면서 실험해 보려는 경우
# None으로 넣어주면 텐서플로가 알아서 계산함.
X = tf.placeholder(tf.float32, [None, 784])
Y = tf.placeholder(tf.float32, [None, 10])


W1 = tf.Variable(tf.random_normal([784, 256], stddev=0.01))
L1 = tf.nn.relu(tf.matmul(X, W1))

# tf.Variable(tf.random_normal([256, 256], stddev=0.01)) 함수는 표준편차가 0.01인 정규분포를 가지는 임의의
# 값으로 뉴런(변수)를 초기화 시킴
W2 = tf.Variable(tf.random_normal([256, 256], stddev=0.01))
L2 = tf.nn.relu(tf.matmul(L1, W2))


# 마지막 계층인 model 텐서에 W3 변수를 곱함으로써 요소 10개짜리 배열이 출력됨.
# 10개의 요소는 0~9까지의 숫자를 나타냄. 가장 큰값을 가진 인덱스가 예측결과에 가까운 숫자
# 출력층에는 보통 활성화 함수 사용하지 않음
W3 = tf.Variable(tf.random_normal([256, 10], stddev=0.01))
model = tf.matmul(L2, W3)

# tf.nn.softmax_cross_entropy_with_logits 함수를 이용하여 각 이미지에 대한 손실값(실제값과 예측값의 차이)를 구하고
# tf.reduce_mean 함수를 이용해 미니배치의 평균 손실값을 구함.
# tf.train.AdamOptimizer 함수를 이용해 이 손실값을 최소화하는 최적화를 수행하도록 그래프를 구성
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))
optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)

# 앞에서 구성한 신경망 모델을 초기화하고 학습을 진행할 세션을 시작
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

# MNIST 는 데이터수가 수만개로 매우 크므로 미니배치 방식으로 학습
batch_size = 100      # 미니배치의 크기는 100개
total_batch = int(mnist.train.num_examples / batch_size)     # mnist.train.num_examples를 배치크기로 나눠
#  미니배치가 총 몇개인지를 저장해둠

# MNIST 데이터 전체를 학습하는 일을 총 15번 반복 ( 학습데이터 전체를 한바퀴 도는 것 = epoch )
for epoch in range(15):
    total_cost = 0

# mnist.train.next_batch -> 학습할 데이터를 배치크기만큼 가져온뒤, 입력값인 이미지 데이터는 batch_xs에
# 출력값인 레이블 데이터는 batch_ys에 저장.
    for i in range(total_batch):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)

# sess.run을 이용해 최적화 시키고 손실값을 가져와 저장
# 이때 feed_dict 매개변수에 입력값 X와 예측을 평가할 실제 레이블값 Y에 사용할 데이터를 넣어줌.
        _, cost_val = sess.run([optimizer, cost])
        feed_dict = {X: batch_xs, Y: batch_ys}
# 그리고 손실값을 저장한후, 한세대의 학습이 끝나면 학습한 세대의 평균 손실값을 출력함.
        total_cost += cost_val

    print('Epoch:', '%04d' % (epoch + 1),
          'Avg. cost = ', '{:.3f}'.format(total_cost / total_batch))

print('최적화 완료')


# 예측결과인 model의 값과 실제 레이블인 Y의 값을 비교
# 예측한 결과값은 원-핫 인코딩 형식이며 각 인덱스에 해당하는 값은 다음처럼 해당숫자가 얼마나 해당인덱스와 관련이 높은가를 나타냄.
# 이것은 손실값을 softmax_cross_entropy_with_logits를 이용해 구했기 떄문.
# tf.argmax(model, 1) 은 두번째 차원(1번 인덱스의 차원)의 값 중 최댓값의 인덱스를 뽑아내는 함수.
# model로 출력한 결과는 [None, 10]처럼 결과값을 배치 크기만큼 가지고 있음
is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))

# tf.cast 이용해 is_correct를 0과 1로 변환 그리고 변환한 값들을 tf.reduce_mean을 이요해 평균을 내면 그것이 바로 정확도(확률)이 됨
accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))

# 테스트 데이터를 다루는 객체인 mnist.test 를 이용해 테스트 이미지와 레이블 데이터를 넣어 accuracy를 계산
print('정확도: ', sess.run(accuracy,
                        feed_dict={X: mnist.test.images,
                                   Y: mnist.test.labels}))





'''드롭아웃'''

# 오버피팅 방지에 굉장히 효과적인 기술: 드롭아웃
# 학습시 전체 신경망중 일부만을 사용하는 방법으로 학습단계마다 일부 뉴런을 사용하지 않도록 함으로써 일부 특징이 특정뉴런들에 고정되는 것을 막아 가중치의 균형을 잡도록 하여 과적합 방지. 다만 학습시 일부 뉴런을 학습시키지 않기 때문에 학습시간은 조금 더 오래걸림.


# 앞에서 만든 손글씨 인식모델에 드롭아웃 기법 적용하기

# 계층구성의 마지막층에 tf.nn.dropout 함수를 사용하면 됨.

W1 = tf.Variable(tf.random_normal([784, 256], stddev=0.01))
L1 = tf.nn.relu(tf.matmul(X, W1))
L1 = tf.nn.dropout(L1, 0.8)

W2 = tf.Variable(tf.random_normal([256, 256], stddev=0.01))
L2 = tf.nn.relu(tf.matmul(L1, W2))
L2 = tf.nn.dropout(L2, 0.8)



# 드롭아웃 기법을 사용해 학습하더라도, 학습이 끝난뒤 예측시에는 신경망 전체를 사용해 주어야함. 따라서 keep_prob라는 플레이스홀더를 만들어 학습시에는 0.8을 넣어 드롭아웃을 사용하고 예측시에는 1을 넣어 신경망 전체를 사용하도록 만듬.

keep_prob = tf.placeholder(tf.float32)

W1 = tf.Variable(tf.random_normal([784, 256], stddev=0.01))
L1 = tf.nn.relu(tf.matmul(X, W1))
L1 = tf.nn.dropout(L1, keep_prob)

W2 = tf.Variable(tf.random_normal([256, 256], stddev=0.01))
L2 = tf.nn.relu(tf.matmul(L1, W2))
L2 = tf.nn.dropout(L2, keep_prob)

# 학습 코드 : keep_prob를 0.8로 넣어줌.
_, cost_val = sess.run([optimizer, cost],
                       feed_dict={X: batch_xs, Y: batch_ys,
                                  keep_prob: 0.8})

# 예측코드 : keep_prob 을 1로 넣어줌
print('정확도: ', sess.run(accuracy, feed_dict={X: mnist.test.images,
                                             Y: mnist.test.labels,
                                             keep_prob: 1}))

# 드롭아웃 기법을 적용한뒤 학습을 진행해보면 드롭아웃을 적용하지 않을떄와 별 차이 없음
# 이유는 드롭아웃 방식으로 인해 학습이 느리게 진행되었기 때문 그렇다면 학습 세대(epoch)를
# 30번으로 늘려 학습을 해보면 결과가 더 좋게 나옴.


''' 배치 정규화 '''
# 이 기법은 과적합을 막아줄 뿐아니라 학습속도도 향상시켜주는 장점으로 드롭아웃보다 자주사용됨.
# tf.nn.batch_nomalization 과 tf.layers.batch_normalization 함수로 쉽게 적용가능



'''matplotlib'''

import matplotlib.pyplot as plt


# 테스트 데이터를 이용해 예측 모델을 실행하고 결괏값을 labels에 저장
labels = sess.run(model, feed_dict={X: mnist.test.images,
                                    Y: mnist.test.labels,
                                    keep_prob: 1})

# 그런 다음 손글씨를 출력할 그래프를 준비
fig = plt.figure()

# 테스트 데이터의 첫번째부터 열번째까지의 이미지와 예측한 값을 출력
for i in range(10):
    # 2행 5열의 그래프를 만들고, i+1번째에 숫자이미지를 출력
    subplot = fig.add_subplot(2, 5, i+1)
    # 이미지를 깨끗하게 출력하기 위해 x와 y의 눈금을 출력하지 않음
    subplot.set_xticks([])
    subplot.set_yticks([])
    # 출력한 이미지 위에 예측한 숫자 출력
    # np.argmax는 tf.argmax와 같은 기능의 함수
    # 결과값인 labels의 i번째 요소가 원-핫 인코딩형식으로 되어있으므로 해당 배열에서 가장 높은 값을 가진 인덱스를 예측한 숫자로 출력
    subplot.set_title('%d' % np.argmax(labels[i]))
    # 1차원 배열로 되어있는 i번째 이미지 데이터를 28x28형식의 2차원 배열로 변형하여 이미지 형태로 출력
    # cmap 파라미터를 통해 이미지를 그레이스케일로 출력
    subplot.imshow(mnist.test.images[i].reshape((28,28)),
                   cmap=plt.cm.gray_r)

plt.show()







'''CNN'''
'''
CNN 모델은 기본적으로 컨볼루션 계층(합성곱 계층)과 풀링계층 으로 구성됨.
이 계층들을 얼마나 많이 또 어떠한 방식으로 쌓느냐에 따라 성능차이와 풀수있는 문제가 달라짐

2D 컨볼루션의 경우 
2차원의 평면 행렬에서 지정한 영역의 값들을 하나의 값으로 압축하는 것
단, 하나의 값으로 압축할때 컨볼루션 계층은 가중치와 편향을 적용하고 풀링계층은 단순히 값들 중 
하나를 선택해서 가져오는 방식

윈도우: 지정한 크기의 영역
윈도우의 값을 오른쪽 그리고 아래쪽으로 한칸씩 움직이면서 은닉층을 완성. 움직이는 크기 또한 변경가능.
몇칸씩 움직일지 정하는 값을 '스트라이드' 라고 함

이렇게 입력층의 윈도우를 은닉층의 뉴런하나로 압축할때, 컨볼루션 계층에서는 윈도우 크기만큼의 가중치와 
1개의 편향을 적용.
ex) 윈도우 크기가 3x3이면 , 3x3 개의 가중치와 1개의 편향이 필요.

3x3 개의 가중치와 1개의 편향을 커널 또는 필터라고 하며, 이 커널은 해당 은닉층을 만들기 위한 모든 윈도우에
공통으로 적용됨

=> 이것이 바로 CNN의 가장 중요한 특징. ex) 입력층이 28x28 개라고 했을때, 기본 신경망으로 모든 뉴련을 연결한다면
784개의 가중치를 찾아내야 하지만 컨볼루션 계층에서는 3x3개인 9개의 가중치만 찾아내면 됨.
따라서 계산량에 매우적어져 학습이 빠르고 효율적으로 이루어짐
but, 이렇게 하면 복잡한 특징을 가진 이미지들을 분석하기에 부족하므로 보통 커널을 여러개 사용
커널의 크기나 개수 역시 하이퍼파라미터의 하나로써 분석하고자 하는 내용에 따라 커널의 개수를 정하는 일 매우 중요
'''


# 앞에서 사용한 MNIST 데이터를 CNN으로 학습시키는 모델 만들기

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data


mnist = input_data.read_data_sets('./mnist/data/', one_hot=True)
# 앞장에서 만든 모델에서는 입력값을 28x28짜리 차원 하나로 구성했지만 CNN 모델에서는 앞서 설명한 것처럼 2차원 평면으로
# 구성하므로 다음처럼 조금더 직관적인 형태로 구성가능

# X의 첫번째 차원인 None은 입력데이터의 개수 그리고 마지막 차원인 1은 특징의 개수로 MNIST 데이터는 회색조 이미지라 채널에 색상이 한개 뿐이므로 1을 사용.
# 그리고 출력값인 10개의 분류와, 드롭아웃을 위한 keep_prob 플레이스 홀더 정의.
X = tf.placeholder(tf.float32, [None, 28, 28, 1])
Y = tf.placeholder(tf.float32, [None, 10])
keep_prob = tf.placeholder(tf.float32)


# 첫번째 CNN 계층 구성
# 3x3 크기의 커널을 가진 컨볼루션 계층 만듬.
# 커널에 사용할 가중치 변수와 tf.nn.conv2d 함수를 사용하면 간단하게 구성가능
W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))

# 입력층 X와 첫번째 계층의 가중치 W1을 가지고 오른쪽과 아래쪽으로 한칸씩 움직이는 32개의 커널을 가진
# 컨볼루션 계층을 만들겠다는 코드
# padding='SAME'은 커널 슬라이딩 시 이미지의 가장 외곽에서 한칸 밖으로 움직이는 옵션
# 이로인해 이미지의 테두리까지도 조금 더 정확하게 평가 가능.
L1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding='SAME')
L1 = tf.nn.relu(L1) # relu 활성화 함수통해 컨볼루션 계층을 완성

# 풀링계층 만들기.
# 이 코드는 앞서 만든 컨볼루션 계층을 입력층으로 사용하고 커널 크기를 2x2로 하는 풀링계층 만듬
# strides=[1,2,2,1] 값은 슬라이딩시 두칸씩 움직이겠다는 옵션
L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')

# 3x3 크기의 컨널 64개로 구성한 컨볼루션 계층과 2x2 크기의 풀링계층으로 구성
# 두번째 컨볼루션 계층의 커널인 W2 변수의 구성은 [3,3,32,64]. 여기에서 32는 앞서 구성한 첫번쨰 계층의 커널개수. 이것은 출력층의 개수이며 첫번째 컨볼루션 계층이 찾아낸 이미지의 특징 개수.
W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))
L2 = tf.nn.conv2d(L1, W2, strides=[1,1,1,1], padding='SAME')
L2 = tf.nn.relu(L2)
L2 = tf.nn.max_pool(L2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')


# 이제 추출한 특징들을 이용해 10개의 분류를 만들어내는 계층 구성
# 먼저 10개의 분류는 1차원 배열이므로 차원을 줄이는 단계를 거쳐야 함.
# 직전의 풀링 계층의 크기가 7x7x64 이므로, 먼저 tf.shape 함수 이용해 7x7x64 크기의 1차원 계층으로 만들고
# 이 배열 전체를 최종 출력값의 중간단계인 256개의 뉴런으로 연결하는 신경망 만들어줌. 
# 참고로 이처럼 인접한 계층의 모든 뉴런과 상호연결된 계층을 '완전연결계층' 이라 함
# 이번계층에는 과적합을 막아주는 드롭아웃 기법 사용
W3 = tf.Variable(tf.random_normal([7 * 7 * 64, 256], stddev=0.01))
L3 = tf.reshape(L2, [-1, 7 * 7 * 64])
L3 = tf.matmul(L3, W3)
L3 = tf.nn.relu(L3)
L3 = tf.nn.dropout(L3, keep_prob)

# 모델 구성의 마지막으로 직전 은닉층인 L3의 출력값을 256개를 받아 최종 출력값인 0~9 레이블을 갖은 10개의 출력값을 만듬
W4 = tf.Variable(tf.random_normal([256,10], stddev=0.010))
model = tf.matmul(L3, W4)

# 손실함수와 AdamOptimizer를 이용한 최적화 함수 만듬
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))
optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)
# optimizer = tf.train.RMSProOptimizer(0.001, 0.9).minimize(cost) 사용가능


# 학습시키고 결과를 확인하는 코드 작성
# 앞에서 구성한 신경망 모델을 초기화하고 학습을 진행할 세션을 시작
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

# MNIST 는 데이터수가 수만개로 매우 크므로 미니배치 방식으로 학습
batch_size = 100      # 미니배치의 크기는 100개
total_batch = int(mnist.train.num_examples / batch_size)    
# mnist.train.num_examples를 배치크기로 나눠 미니배치가 총 몇개인지를 저장해둠

# MNIST 데이터 전체를 학습하는 일을 총 15번 반복 ( 학습데이터 전체를 한바퀴 도는 것 = epoch )
for epoch in range(15):
    total_cost = 0

# mnist.train.next_batch -> 학습할 데이터를 배치크기만큼 가져온뒤, 입력값인 이미지 데이터는 batch_xs에
# 출력값인 레이블 데이터는 batch_ys에 저장.
    for i in range(total_batch):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
        # 모델에 입력값 전달하기 위해 MNIST 데이터를 28x28형태로 재구성
        batch_xs = batch_xs.reshape(-1, 28,28, 1)

# sess.run을 이용해 최적화 시키고 손실값을 가져와 저장
# 이때 feed_dict 매개변수에 입력값 X와 예측을 평가할 실제 레이블값 Y에 사용할 데이터를 넣어줌.
        _, cost_val = sess.run([optimizer, cost],
                               feed_dict = {X: batch_xs, Y: batch_ys,
                                            keep_prob:0.7})
# 그리고 손실값을 저장한후, 한세대의 학습이 끝나면 학습한 세대의 평균 손실값을 출력함.
        total_cost += cost_val

    print('Epoch:', '%04d' % (epoch + 1),
          'Avg. cost = ', '{:.3f}'.format(total_cost / total_batch))

print('최적화 완료')


# 예측결과인 model의 값과 실제 레이블인 Y의 값을 비교
# 예측한 결과값은 원-핫 인코딩 형식이며 각 인덱스에 해당하는 값은 다음처럼 해당숫자가 얼마나 해당인덱스와 관련이 높은가를 나타냄. <- 손실값을 softmax_cross_entropy_with_logits를 이용해 구했기 떄문.
# tf.argmax(model, 1) 은 두번째 차원(1번 인덱스의 차원)의 값 중 최댓값의 인덱스를 뽑아내는 함수.
# model로 출력한 결과는 [None, 10]처럼 결과값을 배치 크기만큼 가지고 있음
is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))

# tf.cast 이용해 is_correct를 0과 1로 변환 그리고 변환한 값들을 tf.reduce_mean을 이요해 평균을 내면 그것이 바로 정확도(확률)이 됨
accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))

# 테스트 데이터를 다루는 객체인 mnist.test 를 이용해 테스트 이미지와 레이블 데이터를 넣어 accuracy를 계산
# 모델에 입력값 전달하기 위해 MNIST 데이터를 28x28형태로 재구성
print('정확도: ', sess.run(accuracy,
                        feed_dict={X: mnist.test.images.reshape(-1,28,28,1),
                                   Y: mnist.test.labels, keep_prob: 1}))



'''고수준 API'''
# tf.layers 등의 고급 API를 사용하면 활성화 함수나 컨볼루션 계층을 만들기 위한 나머지 수치들을 알아서 계산하고 적용함. 또한 가중치를 초기화 할때 기본적으로 xavier_initializer 함수를 사용하는 등 신경망을 효율적으로 만들어 주는 옵션들도 쉽계 사용할수 있도록 해줌


#첫번째 컨볼루션 및 풀링 계층
W1 = tf.Variable(tf.random_normal([3,3,1,32], stddev=0.01))
L1 = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='SAME')
L1 = tf.nn.relu(L1)
L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')

# 위 코드와 같음 tf.layers 모듈 사용하여 간편하고 짧아짐
L1 = tf.layers.conv2d(X, 32, [3,3])
L1 = tf.layers.max_pooling2d(L1, [2,2], [2,2])


# 완전연결계층을 만드는 코드
W3 = tf.Variable(tf.random_normal([7*7*64, 256], stddev=0.01))
L3 = tf.reshape(L2, [-1, 7*7*64])
L3 = tf.matmul(L3, W3)
L3 = tf.nn.relu(L3)

# 위 코드와 같음 tf.layers 모듈 사용하여 간편하고 짧아짐
L3 = tf.contrib.layers.flatten(L2)
L3 = tf.layers.dense(L3, 256, activation=tf.nn.relu)





'''비지도 학습 : Autoencoder'''

''' 
오토인코더는 입력값과 출력값을 같게하는 신경망. 가운데 계층의 노드수가 입력값보다 적은것이 특징
이런 구조로 인해 데이터를 압축하는 효과를 얻고, 이 과정이 노이즈 제거에 매우 효과적임
오토인코더의 핵심은 입력층으로 들어온 데이터를 인코더를 통해 은닉층으로 내보내고, 
은닉층의 데이터를 디코더를 통해 출력층으로 내보낸 뒤, 만들어진 출력값을 입력값과 비슷해지도록 
가중치를 찾아내는 것
'''

import numpy as np
import matplotlib.pyplot as plt

mnist = input_data.read_data_sets('./mnist/data/', one_hot=True)

# 하이퍼파라미터로 사용할 옵션들을 따로 빼내어 코드를 구조화
learning_rate = 0.01
training_epoch = 20
batch_size = 100
n_hidden = 256      # 은닉층 뉴런의 개수
n_input = 28*28     # 입력값의 크기로 MNIST의 이미지 크기가 28x28이므로 784

# 먼저 X의 플레이스홀더 설정. 이모델은 비지도 학습이므로 Y값이 없음
X = tf.placeholder(tf.float32, [None, n_input])

# 오토인코더의 핵심 모델은 인코더와 디코더를 만드는것
# 인코더와 디코더를 만드는 방식에 따라 다양한 오토인코더를 만들수 있음
# 인코더 먼저 만들기
# 맨처음 n_hidden 개의 뉴런을 가진 은닉층 만듬. 가중치와 편향 변수를 원하는 뉴런 개수만큼 설정하고
# 그 변수들을 입력값과 곱하고 더한 뒤 활성화 함수인 sigmoid 함수를 적용
# 이떄 중요한 것은 n_input 값보다 n_hidden 값이 더 작다는 것. 이렇게 하면 입력값을 압축하고 노이즈를
# 제거하면서 입력값의 특징을 찾아냄
W_encode = tf.Variable(tf.random_normal([n_input, n_hidden]))
b_encode = tf.Variable(tf.random_normal([n_hidden]))

encoder = tf.nn.sigmoid(
    tf.add(tf.matmul(X, W_encode), b_encode))


# 디코더 만들기
# 입력값을 은닉층의 크기로, 출력값을 입력층의 크기로 만듬.
W_decode = tf.Variable(tf.random_normal([n_hidden, n_input]))
b_decode = tf.Variable(tf.random_normal([n_input]))

decoder = tf.nn.sigmoid(
    tf.add(tf.matmul(encoder, W_decode), b_decode))


# 그런후 가중치들을 최적화하기위한 손실함수 만듬. 우리가 만드는 기본적인 오토인코더의 목적은 출력값을 입력값과 가장 비슷하게 만드는 것. 그렇게 하면 압축된 은닉층의 뉴런들을 통해 입력값의 특징을 알아낼수 있음. 따라서 입력값인 X를 평가하기 위한 실측값으로 사용하고 디코더가 내보낸 결과값과의 차이를 손실값으로 설정. 그리고 이 값의 차이는 거리함수로 구함.
cost = tf.reduce_mean(tf.pow(X - decoder, 2))


# RMSPropOptimizer 함수를 이용한 최적화 함수 설정
optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)


# 학습시키고 결과를 확인하는 코드 작성
# 앞에서 구성한 신경망 모델을 초기화하고 학습을 진행할 세션을 시작
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)


total_batch = int(mnist.train.num_examples / batch_size)
# mnist.train.num_examples를 배치크기로 나눠 미니배치가 총 몇개인지를 저장해둠

for epoch in range(training_epoch):
    total_cost = 0

# mnist.train.next_batch -> 학습할 데이터를 배치크기만큼 가져온뒤, 입력값인 이미지 데이터는 batch_xs에
# 출력값인 레이블 데이터는 batch_ys에 저장.
    for i in range(total_batch):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
# sess.run을 이용해 최적화 시키고 손실값을 가져와 저장
# 이때 feed_dict 매개변수에 입력값 X에 사용할 데이터를 넣어줌.
        _, cost_val = sess.run([optimizer, cost],
                               feed_dict = {X: batch_xs})
# 그리고 손실값을 저장한후, 한세대의 학습이 끝나면 학습한 세대의 평균 손실값을 출력함.
        total_cost += cost_val

    print('Epoch:', '%04d' % (epoch + 1),
          'Avg. cost = ', '{:.4f}'.format(total_cost / total_batch))

print('최적화 완료')


# 이번에는 결과값을 정확도가 아닌 디코더로 생성해낸 결과를 직관적인 방법으로 확이해보기
# 결과를 확인하는 방법은 다양하지만 여기서는 matplotlib을 이용해 이미지로 출력
# 먼저 총 10개의 테스트 데이터를 가져와 디코더를 이용해 출력값으로 만듬
sample_size = 10
sample = sess.run(decoder,
                  feed_dict={X: mnist.test.images[:sample_size]})

# 그런 다음 numpy 모듈을 이용해 MNIST 데이터를 28x28 크기의 이미지 데이터로 재구성한뒤,
# matplotlib의 imshow 함수이용해 그래프에 이미지로 출력.
# 위쪽에는 입력값의 이미지를 , 아래쪽에는 신경망으로 생성한 이미지 출력
fig, ax = plt.subplot(2, sample_size, figsize=(sample_size, 2))

for i in range(sample_size):
    ax[0][i].set_axis_off()
    ax[1][i].set_axis_off()
    ax[0][i].imshow(np.reshape(mnist.test.images[i], (28,28)))
    ax[1][i].imshow(np.reshape(sample[i], (28,28)))

plt.show()




''' GAN '''

'''
결과물을 생성하는 모델로 서로 대립하는 두 신경망을 경쟁시켜가며 결과물 생성방법 학습
ex) 위조지폐범(생성자)가 경찰을 최대한 속이려고 노력하고 경찰(구분자)은 위조한 지폐를 최대한 감별하려고 노력. 이처럼 위조지폐를 만들고 감별 하려는 경쟁을 통해 서로의 능력이 발전. 거의 구분할수 없을 정도로 진짜 같은 위조지폐를 만들수 있음.
실제 이미지를 주고 구분자에게 이 이미지가 진짜임을 판단하게 함. 그런 다음 생성자를 통해 노이즈로부터 임의의 이미지를 만들고 이것을 다시 같은 구분자를 통해 진짜 이미지인지 판단. 이렇게 생성자는 구분자를 속여 진짜처럼 보이게 하고, 구분자는 생성자가 만든 이미지를 최대한 가짜라고 구분하도록 훈련 하는것이 GAN의 핵심. 이렇게 생성자와 구분자의 경쟁을 통해 결과적으로 생성자는 실제 이미지와 상당히 비슷한 이미지를 생성해 낼수 있음.
이미지 생성 외에도 자연어 문장 생성등에 관한 연구도 진행중.  
'''

# GAN 모델을 이용해 MNIST 손글씨 숫자를 무작위로 생성하는 예제 만들기
#  모델을 확장하여 원하는 숫자에 해당하는 이미지를 생성하는 모델 만들기
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('./mnist/data/', one_hot=True)


# 하이퍼파라미터로 사용할 옵션들을 따로 빼내어 코드를 구조화
total_epoch = 100
learning_rate = 0.0002
batch_size = 100
n_hidden = 256      # 은닉층 뉴런의 개수
n_input = 28*28     # 입력값의 크기로 MNIST의 이미지 크기가 28x28 이므로 784
n_noise = 128       # 생성자의 입력값으로 사용할 노이즈의 크기. 랜덤한 노이즈를 입력하고 그 노이즈에서
                    # 손글씨 이미지를 무작위로 생성하도록 함.

# 플레이스홀드 설정. GAN도 비지도학습이므로 Y를 사용하지 않음
# 다만 구분자에 넣을 이미지가 실제 이미지와 생성한 가짜 이미지 두개. 가짜 이미지는 노이즈에서 생성할 것이므로 노이즈를 입력할 플레이스홀더 Z 추가.
X = tf.placeholder(tf.float32, [None, n_input])
Z = tf.placeholder(tf.float32, [None, n_noise])

# 생성자 신경망에 사용할 변수들을 설정. 첫번째 가중치와 편향은 은닉층으로 출력하기 위한 변수들
# 두번째 가중치와 편향은 출력층에 사용할 변수들. 따라서 두번째 가중치의 변수크기는 실제 이미지의 크기와
# 같아야함. 그 크기가 바로 n_input으로 28x28인 784가 됨.
G_W1 = tf.Variable(tf.random_normal([n_noise, n_hidden], stddev=0.01))
G_b1 = tf.Variable(tf.zeros([n_hidden]))
G_W2 = tf.Variable(tf.random_normal([n_hidden, n_input], stddev=0.01))
G_b2 = tf.Variable(tf.zeros([n_input]))

# 이어서 구분자 신경망에 사용할 변수들 설정. 은닉층은 생성자와 동일하게 구성.
# 구분자는 진짜와 얼마나 가까운가를 판단하는 값으로, 0~1 사이의 값을 출력.
# 따라서 하나의 스칼라 값을 출력하도록 구성.
D_W1 = tf.Variable(tf.random_normal([n_input, n_hidden], stddev=0.01))
D_b1 = tf.Variable(tf.zeros([n_hidden]))
D_W2 = tf.Variable(tf.random_normal([n_hidden, 1], stddev=0.01))
D_b2 = tf.Variable(tf.zeros([1]))

# 실제 이미지를 판별하는 구분자 신경망과 생성한 이미지를 판별하는 구분자 신경망은 같은 변수를 사용해야함.
# 같은 신경망으로 구분을 시켜야 진짜 이미지와 가짜 이미지를 구분하는 특징들을 동시에 잡아낼수 있음.

# 이제 생성자와 구분자 신경망을 구성
# 먼저 생성자 신경망
# 생성자 무작위로 생성한 노이즈를 받아 가중치와 편향을 반영하여 은닉층을 만들고 은닉층에서 실제 이미지와
# 같은 크기의 결괏값을 출력하는 간단한 구성.
def generator(noise_z):
    hidden = tf.nn.relu(
        tf.matmul(noise_z, G_W1) + G_b1)
    output = tf.nn.sigmoid(
        tf.matmul(hidden, G_W2) + G_b2)
    return output

# 구분자 신경망은 0~1 사이의 스칼라값 하나를 출력하도록 하였으며, 이를 위한 활성화 함수로 sigmoid함수 사용
def discriminator(inputs):
    hidden = tf.nn.relu(
        tf.matmul(inputs, D_W1) + D_b1)
    output = tf.nn.sigmoid(
        tf.matmul(hidden, D_W2) + D_b2)
    return output

# 무작위한 노이즈를 만들어주는 함수 만들어줌
def get_noise(batch_size, n_noise):
    return np.random.normal(size=(batch_size, n_noise))

# 노이즈 Z를 이용해 가짜 이미지를 만들 생성자 G를 만들고, 이 G가 만든 가짜 이미지와 진짜 이미지 X를
# 각각 구분자에 넣어 입력한 이미지가 진짜인지를 판별하도록 함.
# 이들을 잘 학습시키면 생성자가 실제에 가까운 이미지를 만들수 있게 함.
G = generator(Z)
D_gene = discriminator(G)
D_real = discriminator(X)

# 다음으로는 손실값을 구해야 하는데 이번에는 두개가 필요. 즉, 생성자가 만든 이미지를 구분자가 가짜라고 판단하도록 하는 손실값 (경찰 학습용) 과 진짜라고 판단하도록 하는 손실값 (위조 지폐범 학습용)을 구해야하 함.
# 경찰을 학습시키려면 진짜 이미지 판별값 D_real은 1에 가까워야 하고 (진짜라고 판별), 가짜이미지 판별값 D_gene는 0에 가까워야 함.(가짜라고 판별)
loss_D = tf.reduce_mean(tf.log(D_real) + tf.log(1 - D_gene))

# 간단하게 설명해 'D_real'과 '1에서 D_gene을 뺀 값'을 더한 값을 손실값으로 하며 이 값을 최대화 하면 경찰학습이 이루어짐
# 다음으로 위조지폐범 학습은 가짜 이미지 판별값 D_gene을 1에 가깝게 만들기만 하면 됨.
# 즉, 가짜 이미지를 넣어도 진짜 같다고 판별. 다음과 같이 D_gene을 그대로 넣어 이를 손실값으로 하고, 이값을 최대화하면 위조 지폐범을 학습시킬수 있음.
loss_G = tf.reduce_mean(tf.log(D_gene))

# 즉, GAN의 학습은 loss_D와 loss_G 모두를 최대화 하는 것. 다만 loss_D와 loss_G는 서로 연관되어 있어
# 두 손실값이 항상 같이 증가하는 경향을 보이지는 않음. loss_D가 증가하려면 loss_G는 하락해야 하고
# loss_D는 하락해야 하는 경쟁 관계기 때문.


# 이제 이 손실값들을 이용해 학습시키는 일만 남았음. 이때 주의할 것이 하나 있음.
# loss_D를 구할때는 구분자 신경망에 사용되는 변수들만 사용하고, loss_G를 구할 때는 생성자 신경망에 사용되는 변수들만 사용하여 최적화해하함.
# 그래야 loss_D를 학습할 때는 생성자가 변하지 않고, loss_G를 학습할 때 구분자가 변하지 않기 때문
D_var_list = [D_W1, D_b1, D_W2, D_b2]
G_var_list = [G_W1, G_b1, G_W2, G_b2]

# 이어서 변수들을 최적화하는 함수들을 구성. GAN 논문에 따르면 loss를 최대화 해야 하지만 최적화에 쓸 수있는 함수는 mininmize 뿐이므로 최적화하려면 loss_D 와 loss_G에 음수부호를 붙여줌
train_D = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D, var_list=D_var_list)
train_G = tf.train.AdamOptimizer(learning_rate).minimize(-loss_G, var_list=G_var_list)


# 이것으로 준비는 끝나고 학습을 시키는 코드 작성
# GAN 모델은 두개의 손실값을 학습시켜야함
sess = tf.Session()
sess.run(tf.global_variables_initializer())

total_batch = int(mnist.train.num_examples / batch_size)
loss_val_D, loss_val_G = 0, 0       # loss_D와 loss_G의 결과값을 받을 변수 설정

# 그런 후 미니배치로 반복 학습. 여기서 구분자는 X 값을, 생성자는 노이즈인 Z 값을 받으므로 노이즈를 생성해주는 get_noise 함수를 통해 배치 크기만큼 노이즈를 만들고 이를 입력해줌.
# 그리고 구분자와 생성자 신경망을 각각 학습시킴
for epoch in range(total_batch):
    for i in range(total_batch):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
        noise = get_noise(batch_size, n_noise)

        _, loss_val_D = sess.run([train_D, loss_D],
                                 feed_dict={X: batch_xs, Z: noise})
        _, loss_val_G = sess.run([train_G, loss_G],
                                 feed_dict={Z: noise})
    print('Epoch: ', '%04d' % epoch,
          'D loss: {:.4}'.format(loss_val_D),
          'G loss: {:.4}'.format(loss_val_G))

# GAN 모델을 완성하였으니 학습결과를 확인하는 코드 작성
# 학습이 잘되는지 0번째, 9번째, 19번쨰,29번쨰.. 마다 생성기로 이미지를 생성하여 눈으로 직접 확인함.
# 결과를 확인하는 코드는 학습 루프안에 작성 노이즈를 만들고 이것을 생성자 G에 넣어 결과값 만든뒤
if epoch == 0 or (epoch + 1) % 10 == 0:
    sample_size = 10
    noise = get_noise(sample_size, n_noise)
    samples = sess.run(G, feed_dict={Z: noise})
    #이 결과값들을 28x28 크기의 가짜 이미지로 만들어 samples 폴더에 저장. (samples 폴더는 미리 만들어져있어야함)
    fig, ax = plt.subplots(1, sample_size, figsize=(sample_size, 1))

    for i in range(sample_size):
        ax[i].set_axes_off()
        ax[i].imshow(np.reshape(samples[i], (28, 28)))

    plt.savefig('samples/{}.png'.format(str(epoch).zfill(3)),
                bbox_inches='tight')
    plt.close(fig)

print('최적화 완료!')

# 실행결과 : 노이즈가 무작위로 만들어지니 매번 조금씩 다른 이미지가 생성됨.
# 중요한 점은 여기서 생성된 이미지들은 MNIST 데이터에 없는 새로운 손글씨 이미지 라는 것
